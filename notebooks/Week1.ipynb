{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download the Data using the command line terminal wget\n",
    "* Referal to learn with Machine Learning Zoomcamp 2:23\n",
    "* Read Data and understanding different columns of the data using df.head and data dictionary 3:06\n",
    "* * \n",
    "* Calulate the duration of each trip fropoff - pickup time.\n",
    "* * convert value to pandas date time and calculate the difference in value\n",
    "* * Prprocess the created duration column into minutes\n",
    "* * Visuale distribution and data describe of duration 9:50\n",
    "* * Filter duration that is within 1 and 60 mins  13:10\n",
    "* * Specify catergorical and numeric variables\n",
    "* if you do not have pyarrow installed or fastparquet, you will have to install one\n",
    "** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the location of the file\n",
    "green_trip_data_Jan_2021 = 'data/green_tripdata_2021-01.parquet'\n",
    "green_trip_data_Feb_2021 = 'data/green_tripdata_2021-02.parquet'\n",
    "\n",
    "fhv_trip_data_Jan_2021 = 'data/fhv_tripdata_2021-01.parquet'\n",
    "fhv_trip_data_Feb_2021 = 'data/fhv_tripdata_2021-02.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(filename, pickup_time, drop_off_time, pu_id, do_id):\n",
    "    df = pd.read_parquet(filename)\n",
    "    df['duration'] = pd.to_datetime(df[drop_off_time]) - pd.to_datetime(df[pickup_time])  # calculate duration and convert to minutes\n",
    "    df['duration'] = df['duration'].apply(lambda td: td.total_seconds()/60)\n",
    " \n",
    "    df = df[((df.duration>=1) & (df.duration<=60))]       # filter to only duration within and less than hour\n",
    "\n",
    "\n",
    "\n",
    "    categorical = [pu_id, do_id]\n",
    "\n",
    "    df[categorical] = df_Jan[categorical].fillna(-1)\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Read the data for January. \n",
    "How many records are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1154112, 7)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Jan = pd.read_parquet(fhv_trip_data_Jan_2021)\n",
    "df_Jan.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Computing duration\n",
    "Now let's compute the `duration` variable. It should contain the duration of a ride in minutes.\n",
    "\n",
    "What's the average trip duration in January?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.1672240937939"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Jan['duration'] = pd.to_datetime(df_Jan['dropOff_datetime'])- pd.to_datetime(df_Jan['pickup_datetime'])\n",
    "df_Jan['duration'] = df_Jan['duration'].apply(lambda x: x.total_seconds()/60)\n",
    "df_Jan['duration'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "Check the distribution of the duration variable. There are some outliners.\n",
    "\n",
    "Let's remove them and keep only the records where the duration was between 1 and 60 minutes (inclusive).\n",
    "\n",
    "How many records did you drop?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Jan = df_Jan[((df_Jan.duration>=1) & (df_Jan.duration<=60))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Missing values\n",
    "The features we'll use for our model are the pickup and dropoff location IDs.\n",
    "\n",
    "But they have a lot of missing values there. Let's replace them with \"-1\"\n",
    "\n",
    "What's the factions of missing values for the pickup location ID? (Or the fraction of \"-1\"s after you filled the NAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PUlocationID    0.835273\n",
       "DOlocationID    0.133270\n",
       "dtype: float64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_col = ['PUlocationID', 'DOlocationID']\n",
    "df_Jan[train_col].isnull().sum()/df_Jan.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing value \n",
    "df_Jan[train_col] = df_Jan[train_col].fillna(-1)\n",
    "df_Jan[train_col] = df_Jan[train_col].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. One-hot encoding\n",
    "Let's apply one-hot encoding to the pickup and dropoff location IDs. We'll use only these two features for our model.\n",
    "\n",
    "* Turn the dataframe into a list of dictionaries\n",
    "* Fit a dictionary vectorizer\n",
    "* Get a feature matrix from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_train = df_Jan[train_col].to_dict(orient='records')\n",
    "dv = DictVectorizer()\n",
    "X_train = dv.fit_transform(dict_train) #this returns a sparse cmr matrix\n",
    "Y_train = df_Jan['duration'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1109826, 525)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Training a model\n",
    "Now let's use the feature matrix from the previous step to train a model.\n",
    "\n",
    "* Train a plain linear regression model with default parameters\n",
    "* Calculate the RMSE of the model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.5285191072072"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = lr.predict(X_train) # Make prediciton on the train\n",
    "mean_squared_error(Y_train, Y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Evaluating the model\n",
    "Now let's apply this model to the validation dataset (Feb 2021).\n",
    "\n",
    "What's the RMSE on validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_dataframe(fhv_trip_data_Jan_2021,'pickup_datetime',  'dropOff_datetime', pu_id='PUlocationID', do_id='DOlocationID')\n",
    "df_val = read_dataframe(fhv_trip_data_Feb_2021,'pickup_datetime',  'dropOff_datetime', pu_id='PUlocationID', do_id='DOlocationID')\n",
    "\n",
    "# dv = DictVectorizer()\n",
    "train_dicts = df_val[train_col].to_dict(orient='records')\n",
    "X_val = dv.transform(train_dicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(990113, 525)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.923643749124812"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = lr.predict(X_val) # Make prediciton on the train\n",
    "mean_squared_error(df_val.duration.values, Y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda install -c conda-forge fastparquet\n",
    "# ! conda install -c conda-forge pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('models/lin_reg.bin', 'wb') as f_out:\n",
    "    pickle.dump((dv, lr),f_out)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
